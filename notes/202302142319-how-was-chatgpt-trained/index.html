<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@thomasantony" />
    <meta name="twitter:creator" content="@thomasantony" />
    <meta name="og:url" content="https://www.thomasantony.com/notes/202302142319-how-was-chatgpt-trained/" />
    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="Thomas Antony" />
    <title>How was ChatGPT trained?</title>
    <meta name="og:title" content="How was ChatGPT trained?" />
    <meta name="description" content="My personal website and notes">
    <meta name="og:description" content="My personal website and notes" />
    <link rel="stylesheet" href="/normalize.css" />
    <link rel="stylesheet" href="/style.css" media="screen" />
    <link rel="stylesheet" href="/style-dark.css" media="screen and (prefers-color-scheme: dark)" />
    <title>How Was Chatgpt Trained? - Thomas Antony</title>

</head>
<body>
  <header>
    <div id="nav-brand-wrapper">
      <a class="sakura-blossom sans" href="/">Thomas Antony</a>
    </div>
    <nav>
      
      [
        
        <a class="sakura-fade" href="https://www.thomasantony.com/about">about</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/posts">posts</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/projects">projects</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/notes">notes</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/chatgpt">chatgpt</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/publications">publications</a>
      ]
      
    </nav>
  </header>
  <section class="main">
  
<section>
<p id="post">
  <h1>How was ChatGPT trained?</h1>
  <article><ol>
<li>
<p>Train a GPT-like model to “understand langauge”. This could be based on a data-set of prompts and expected responses.</p>
</li>
<li>
<p>Sample several outputs from the model for a given prompt. Have human labeler rank the outputs. Train yet <em>another</em> transformer based model (the “reward model”) that can predict this rank/“goodness” of the answer based on the human labeled answers.</p>
</li>
<li>
<p>Stack the reward model (RM) at the end of GPT and use it to generate the loss function. This is then used to fine-tune the GPT while keeping the RM frozen. And thus you get ChatGPT.</p>
</li>
</ol>
<h2 id="references">References</h2>
<p>[1] https://www.youtube.com/watch?v=_MPJ3CyDokU</p>
<p>[1] “Learning to summarize from human feedback” https://arxiv.org/pdf/2009.01325.pdf</p>
</article>
</p>
<p>
  <h2>Backlinks</h2>
  
  
  
  
  <li><a href="https:&#x2F;&#x2F;www.thomasantony.com&#x2F;notes&#x2F;computer-science&#x2F;">Computer Science</a></li>
  
  
  <li><a href="https:&#x2F;&#x2F;www.thomasantony.com&#x2F;notes&#x2F;machine-learning&#x2F;">Machine Learning</a></li>
  
  
  <li><a href="https:&#x2F;&#x2F;www.thomasantony.com&#x2F;notes&#x2F;ai&#x2F;">AI</a></li>
  
  
</p>
</section>



  </section>
  <footer class="sans">
    <a href="https://github.com/thomasantony/thomasantony.github.io-src" target="_blank"> Source code</a>. Built using <a href="https://getzola.org" target="_blank">Zola</a> and <a href="https://github.com/oxalorg/sakura" target="_blank">Sakura</a>.
  </footer>
</body>
</html>
