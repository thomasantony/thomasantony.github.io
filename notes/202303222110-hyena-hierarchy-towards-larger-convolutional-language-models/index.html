<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@thomasantony" />
    <meta name="twitter:creator" content="@thomasantony" />
    <meta name="og:url" content="https://www.thomasantony.com/notes/202303222110-hyena-hierarchy-towards-larger-convolutional-language-models/" />
    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="Thomas Antony" />
    <title>Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
    <meta name="og:title" content="Hyena Hierarchy: Towards Larger Convolutional Language Models" />
    <meta name="description" content="My personal website and notes">
    <meta name="og:description" content="My personal website and notes" />
    <link rel="stylesheet" href="/normalize.css" />
    <link rel="stylesheet" href="/style.css" media="screen" />
    <link rel="stylesheet" href="/style-dark.css" media="screen and (prefers-color-scheme: dark)" />
    <title>Hyena Hierarchy: Towards Larger Convolutional Language Models - Thomas Antony</title>

    <!-- MathJax configuration -->
    <script src="https://www.thomasantony.com/js/math.js" type="text/javascript"></script>
    <script async defer src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML,Safe"> </script>

</head>
<body>
  <header>
    <div id="nav-brand-wrapper">
      <a class="sakura-blossom sans" href="/">Thomas Antony</a>
    </div>
    <nav>
      
      [
        
        <a class="sakura-fade" href="https://www.thomasantony.com/about">about</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/posts">posts</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/projects">projects</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/notes">notes</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/chatgpt">chatgpt</a>,
        <a class="sakura-fade" href="https://www.thomasantony.com/publications">publications</a>
      ]
      
    </nav>
  </header>
  <section class="main">
  
<section>
<p id="post">
  <h1>Hyena Hierarchy: Towards Larger Convolutional Language Models</h1>
  <article><p>Here are some rough notes on the “Hyena Hierarchy” architecture described in the paper <a rel="noopener" target="_blank" title="Hyena Hierarchy: Towards Larger Convolutional Language Models" href="https://arxiv.org/abs/2302.10866">1</a>.</p>
<ul>
<li>
<p>This is a new way of getting sub-quadratic scaling for attention</p>
</li>
<li>
<p>it uses convolution filter</p>
<ul>
<li>typical convolution filters are in the form of an array of values which are learned and applied like an Finite-Impulse-Response discrete filter (FIR)<a rel="noopener" target="_blank" title="Finite Impulse Response" href="https://en.wikipedia.org/wiki/Finite_impulse_response">2</a></li>
<li>this doesn’t scale well</li>
<li>instead the filter parameters are represented as a function of “t” where “t” represents the index or “time-step” in the filter. This means you can get a filter of any length from a limited number of parameters</li>
<li>furthermore, this function is chosen to be the output of a state-space model of the type from control theory (Ax+Bu, Cx+Du etc.)</li>
<li>If x0 = 0, then you can get an expression for the output “y” (aka the filter), in terms of matrices A, B, C and D (which can be learned during training)</li>
<li>dimensions of the state-space model and structore of the matrices represent the degrees of freedom available</li>
</ul>
</li>
<li>
<p>FFT can be used to implement convolutions</p>
</li>
<li>
<p>Typical attention involves three linear projections passed through a softmax function - called query, key and value</p>
</li>
<li>
<p>“Hyena” uses N+1 linear projections (not necessarily equal to three). One of these projections take the role of the “value”.</p>
</li>
<li>
<p>So <code>y = H(u)v</code></p>
<ul>
<li>H(u) is defined by “interleaving implicit long convolutions and element-wise multiplication” with one projection at a time</li>
<li>It somehow retains the sublinear scaling by not “materializing” H(u)</li>
<li>The element-wise product in time domain corresponds to convolution in frequency domain</li>
</ul>
</li>
<li>
<p>more details to come as I understand the paper further</p>
</li>
</ul>
<h2 id="references">References</h2>
<p>[<a rel="noopener" target="_blank" title="Hyena Hierarchy: Towards Larger Convolutional Language Models" href="https://arxiv.org/abs/2302.10866">1</a>] “Hyena Hierarchy: Towards Larger Convolutional Language Models”, Poli et.al, https://arxiv.org/abs/2302.10866</p>
<p>[<a rel="noopener" target="_blank" title="Finite Impulse Response" href="https://en.wikipedia.org/wiki/Finite_impulse_response">2</a>] “Finite-Impulse Response”, https://en.wikipedia.org/wiki/Finite_impulse_response</p>
</article>
</p>
<p>
  <h2>Backlinks</h2>
  
  
  
  
  <li><a href="https:&#x2F;&#x2F;www.thomasantony.com&#x2F;notes&#x2F;ai-ml&#x2F;">AI&#x2F;ML</a></li>
  
  
  <li><a href="https:&#x2F;&#x2F;www.thomasantony.com&#x2F;notes&#x2F;ai-research-papers&#x2F;">AI Research Papers</a></li>
  
  
</p>
</section>



  </section>
  <footer class="sans">
    <a href="https://github.com/thomasantony/thomasantony.github.io-src" target="_blank"> Source code</a>. Built using <a href="https://getzola.org" target="_blank">Zola</a> and <a href="https://github.com/oxalorg/sakura" target="_blank">Sakura</a>.
  </footer>
</body>
</html>
