<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>thomasantony.com - AI Research Papers</title>
	<subtitle>My personal website and notes</subtitle>
	<link href="https://www.thomasantony.com/notes/ai-research-papers/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://www.thomasantony.com"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2023-04-23T04:10:30.070+00:00</updated>
	<id>https://www.thomasantony.com/notes/ai-research-papers/atom.xml</id>
	<entry xml:lang="en">
		<title>Hyena Hierarchy: Towards Larger Convolutional Language Models</title>
		<published>2023-04-23T04:10:30.070+00:00</published>
		<updated>2023-04-23T04:10:30.070+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202303222110-hyena-hierarchy-towards-larger-convolutional-language-models/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202303222110-hyena-hierarchy-towards-larger-convolutional-language-models/</id>
		<content type="html">&lt;p&gt;Here are some rough notes on the “Hyena Hierarchy” architecture described in the paper &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Hyena Hierarchy: Towards Larger Convolutional Language Models&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.10866&quot;&gt;1&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This is a new way of getting sub-quadratic scaling for attention&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;it uses convolution filter&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;typical convolution filters are in the form of an array of values which are learned and applied like an Finite-Impulse-Response discrete filter (FIR)&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Finite Impulse Response&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Finite_impulse_response&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;this doesn’t scale well&lt;&#x2F;li&gt;
&lt;li&gt;instead the filter parameters are represented as a function of “t” where “t” represents the index or “time-step” in the filter. This means you can get a filter of any length from a limited number of parameters&lt;&#x2F;li&gt;
&lt;li&gt;furthermore, this function is chosen to be the output of a state-space model of the type from control theory (Ax+Bu, Cx+Du etc.)&lt;&#x2F;li&gt;
&lt;li&gt;If x0 = 0, then you can get an expression for the output “y” (aka the filter), in terms of matrices A, B, C and D (which can be learned during training)&lt;&#x2F;li&gt;
&lt;li&gt;dimensions of the state-space model and structore of the matrices represent the degrees of freedom available&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;FFT can be used to implement convolutions&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Typical attention involves three linear projections passed through a softmax function - called query, key and value&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;“Hyena” uses N+1 linear projections (not necessarily equal to three). One of these projections take the role of the “value”.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;So &lt;code&gt;y = H(u)v&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;H(u) is defined by “interleaving implicit long convolutions and element-wise multiplication” with one projection at a time&lt;&#x2F;li&gt;
&lt;li&gt;It somehow retains the sublinear scaling by not “materializing” H(u)&lt;&#x2F;li&gt;
&lt;li&gt;The element-wise product in time domain corresponds to convolution in frequency domain&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;more details to come as I understand the paper further&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Hyena Hierarchy: Towards Larger Convolutional Language Models&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.10866&quot;&gt;1&lt;&#x2F;a&gt;] “&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2302.10866&quot;&gt;Hyena Hierarchy: Towards Larger Convolutional Language Models&lt;&#x2F;a&gt;”, Poli et.al&lt;&#x2F;p&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Finite Impulse Response&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Finite_impulse_response&quot;&gt;2&lt;&#x2F;a&gt;] &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Finite_impulse_response&quot;&gt;Finite-Impulse Response&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Generalization through memorization: Nearest neighbor language models</title>
		<published>2023-04-13T02:25:07.127+00:00</published>
		<updated>2023-04-13T02:25:07.127+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202303121925-paper-generalization-through-memorization-nearest-neighbor-language-models/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202303121925-paper-generalization-through-memorization-nearest-neighbor-language-models/</id>
		<content type="html">&lt;p&gt;This paper&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Generalization through memorization: Nearest neighbor language models&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1911.00172.pdf&quot;&gt;1&lt;&#x2F;a&gt; describes a method for augmenting an existing language model with external memory to improve its performance without requiring any extra training. The datastore is initialized for a given dataset and used during inference time. The authors demonstrate performance improvements (measured in perplexity over a given dataset) even when operating on data-stores for datasets the model was not trained on.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-it-works&quot;&gt;How it works&lt;&#x2F;h2&gt;
&lt;p&gt;The method essentially involves “memorizing” the training set and using it to directly augment the model at inference time. This can also be used for memorizing data &lt;em&gt;other than the training set&lt;&#x2F;em&gt; and give similar improvements.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;indexing-or-memorizing&quot;&gt;Indexing or “Memorizing”&lt;&#x2F;h3&gt;
&lt;p&gt;The data-store is a “key-value” database, similar to &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;github.com&#x2F;facebookresearch&#x2F;faiss&quot;&gt;faiss&lt;&#x2F;a&gt; or &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.pinecone.io&#x2F;&quot;&gt;pinecone&lt;&#x2F;a&gt; with vectors (of floating point numbers) that form keys and some arbitrary data as value. To “memorize” a data set, the existing LLM is evaluated on the data split into some chunk size, and the outputs of the network right before the final “softmax” layer is used as the “key” for the database, while the subsequent token in the dataset (which the model is supposed to predict) is stored as the value. &lt;&#x2F;p&gt;
&lt;p&gt;Assume that $x_0$, $x_1$ … $x_{n-1}$, are the different tokens in a chunk of text from the dataset. Let &lt;code&gt;f&lt;&#x2F;code&gt; be a function that converts this token sequence into the “key” vector. In the paper, they examined different layer outputs for this and showed that the output of the final layer, right before the soft-max activation, is a good candidate for this. So for a given token-sequence, the key and value are given by&lt;&#x2F;p&gt;
&lt;div&gt;
    $$
k = f([x_0, x_1, ... , x_{n-1}])\\
v = x_n
$$
&lt;&#x2F;div&gt;
&lt;p&gt;For a chunk of n-tokens, we may have up to n-1 different data-points stored in the database, i.e. $f([x_0]) \rightarrow x_1$, $f([x_0, x_1]) \rightarrow x_2$ and so on.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;&#x2F;h3&gt;
&lt;p&gt;At inference time, the input token-sequence is run through the LM to get the probability distribution for the next token. The activations of the final layer prior to the soft-max is then used to perform a k-nearest-neighbors search in the vector datastore created in the last step (the authors used Euclidean distance and k=1000 for this search). The vectors are then converted into a probability distribution of its own using the distances as follows:&lt;&#x2F;p&gt;
&lt;div&gt;
    $$
P_{kNN}(y|x) = \sum_{(k_i, v_i \in N)} \mathbb{1}_{y=v_i} \exp\left(\ -d(k_i, f(x_i)) \right)
$$
&lt;&#x2F;div&gt;
&lt;p&gt;where $\mathbb{1}_{y=v_i}$ is the one-hot encoded vector for token $v_i$, $d(k_i, f(x_i))$ is the distance for the search result from the search-key.&lt;&#x2F;p&gt;
&lt;p&gt;$P_{LM}(y|x)$ are the logits from the original model. The final probability distribution is then computed by linearly interpolating between the two distributions:&lt;&#x2F;p&gt;
&lt;div&gt;
    $$
P(y|x) = \lambda P_{kNN}(y|x) + (1 - \lambda) P_{LM}(y|x)
$$
&lt;&#x2F;div&gt;
&lt;p&gt;where $\lambda$ is a fixed coefficient. Reference &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;You can&#x27;t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.15859&quot;&gt;2&lt;&#x2F;a&gt; talks about selecting $\lambda$ based on “semantic similarity” (the cosine distance?) between the closest key from the search results and the search query. They trained a model to predict what the coefficient-profile should be for a given dataset (i.e how to map semantic similarity to the interpolation coefficient $\lambda$).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Generalization through memorization: Nearest neighbor language models&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1911.00172.pdf&quot;&gt;1&lt;&#x2F;a&gt;] Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., &amp;amp; Lewis, M. (2019). Generalization through memorization: Nearest neighbor language models. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1911.00172.pdf&quot;&gt;arXiv preprint arXiv:1911.00172&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;You can&#x27;t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.15859&quot;&gt;2&lt;&#x2F;a&gt;] Drozdov, A., Wang, S., Rahimi, R., McCallum, A., Zamani, H., &amp;amp; Iyyer, M. (2022). You can’t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM. &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2210.15859&quot;&gt;arXiv preprint arXiv:2210.15859&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>
