<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
	<title>Thomas Antony - Computer Science</title>
	<subtitle>My personal website and notes</subtitle>
	<link href="https://www.thomasantony.com/notes/computer-science/atom.xml" rel="self" type="application/atom+xml"/>
  <link href="https://www.thomasantony.com"/>
	<generator uri="https://www.getzola.org/">Zola</generator>
	<updated>2022-11-04T01:54:02.252+00:00</updated>
	<id>https://www.thomasantony.com/notes/computer-science/atom.xml</id>
	<entry xml:lang="en">
		<title>Seam carving algorithm</title>
		<published>2022-11-04T01:54:02.252+00:00</published>
		<updated>2022-11-04T01:54:02.252+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202210031854-seam-carving-algorithm/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202210031854-seam-carving-algorithm/</id>
		<content type="html">&lt;p&gt;Seam carving is an algorithm that can be used to resize an image while removing the least important pixels from it. &lt;&#x2F;p&gt;
&lt;p&gt;One possible implementation is to use an edge-detection algorithm first to create an edge-map[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Seam Carving | Week 2 | 18.S191 MIT Fall 2020 | Grant Sanderson&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rpB6zQNsbQU&quot;&gt;1&lt;&#x2F;a&gt;]. This is followed by finding continuous lines that run from top to the bottom, where the only directions of movement allowed are “south”, “south-east” and “south-west”, that minimize the sum of “edge” values. This can be solved using a dynamic programming approach of starting at the bottom and assigning the “minimum-cost-to-bottom” to each cell.&lt;&#x2F;p&gt;
&lt;p&gt;Once this is done, we can find the path throug hthe image with the least cost and remove it. &lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Seam Carving | Week 2 | 18.S191 MIT Fall 2020 | Grant Sanderson&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rpB6zQNsbQU&quot;&gt;1&lt;&#x2F;a&gt;] “Seam Carving | Week 2 | 18.S191 MIT Fall 2020 | Grant Sanderson”&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Two&#x27;s Complement Notation for Binary Addition</title>
		<published>2022-09-04T03:41:18.681+00:00</published>
		<updated>2022-09-04T03:41:18.681+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202208032041-two-s-complement-notation-for-binary-addition/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202208032041-two-s-complement-notation-for-binary-addition/</id>
		<content type="html">&lt;p&gt;Two’s complement is a format for representing negative numbers in binary. A number &lt;code&gt;-X&lt;&#x2F;code&gt; is represented as an &lt;code&gt;N&lt;&#x2F;code&gt; bit value using the binary value of &lt;code&gt;2^N-X&lt;&#x2F;code&gt;. E.g. -5 in 4-bit integers would be &lt;code&gt;2^4 - 5&lt;&#x2F;code&gt; = &lt;code&gt;11&lt;&#x2F;code&gt; (or &lt;code&gt;0b1010&lt;&#x2F;code&gt;). The advantage of using this representation is that it makes addition of negative numbers possible with existing binary adder circuits (or logic gate implementation). The reason this works is that the output we get from an adder circuit is actually the “modulo 2^n” representation. &lt;&#x2F;p&gt;
&lt;p&gt;Example: -2 + -3 using 4-bit integers&lt;&#x2F;p&gt;
&lt;p&gt;-2 in 2’s complement is 2^4 - 2 = 16-2 = 14 or &lt;code&gt;0b1110&lt;&#x2F;code&gt;
-3 in 2’s complement is 2^4 - 3 = 16-3 = 13 or &lt;code&gt;0b1101&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Performing binary addition of &lt;code&gt;0b1110&lt;&#x2F;code&gt; and &lt;code&gt;0b1101&lt;&#x2F;code&gt; we get, &lt;code&gt;0b1011&lt;&#x2F;code&gt; and a carry bit of 1 which is discarded. &lt;code&gt;0b1011&lt;&#x2F;code&gt; is &lt;code&gt;11&lt;&#x2F;code&gt; in decimal. This is equal to &lt;code&gt;-5&lt;&#x2F;code&gt; in 2’s complement representation which is the correct answer.&lt;&#x2F;p&gt;
&lt;p&gt;In order to do subtractions using a digital adder circuit, it becomes necessary to compute “-x” given a value “x”. This is done as follows:&lt;&#x2F;p&gt;
&lt;p&gt;Negative value of X in 2’s complement notation is given by &lt;&#x2F;p&gt;
&lt;p&gt;2^n - x = 1 + 2^n - 1 - x = 1 + (2^n -1 ) - x
= 1 + (0b111111… - x)
= 1 + ~X where ~ is the bitwise inverse
= ~(X + 0b11111…) (from ALU truth table)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;other-properties&quot;&gt;Other properties&lt;&#x2F;h2&gt;
&lt;p&gt;(x + 1) = ~(~x + 0b1111….)
(x - 1) = x + 0b1111….
(x - y) = ~(~x + y)
(y - x) = ~(x + y)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[1] Nand2Tetris Unit 2.3
[2]: https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Two%27s_complement#Addition
[[2]] &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Two%27s_complement#Addition&quot;&gt;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Two%27s_complement#Addition&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>What is TLA+?</title>
		<published>2022-06-10T03:31:49.999+00:00</published>
		<updated>2022-06-10T03:31:49.999+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202205092031-what-is-tla/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202205092031-what-is-tla/</id>
		<content type="html">&lt;p&gt;TLA+ is a language for creating “specifications” for computer programs&#x2F;algorithms and then verify all the possible execution paths and mathematically prove assertions. The program is represented as a state machine where everything from variables to the program counter “pc” is part of the state. The spec contains an initial state and rule specifying “next state” as a bunch of logical statements linked by “and” (&#x2F;) or “or” aka &#x2F; clauses. &lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;The Man Who Revolutionized Computer Science With Math&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rkZzg7Vowao&quot;&gt;1&lt;&#x2F;a&gt;] https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=rkZzg7Vowao&lt;&#x2F;p&gt;
&lt;p&gt;[&lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; title=&quot;Introduction to TLA+&quot; href=&quot;https:&#x2F;&#x2F;lamport.azurewebsites.net&#x2F;video&#x2F;intro.html&quot;&gt;2&lt;&#x2F;a&gt;] &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;lamport.azurewebsites.net&#x2F;video&#x2F;intro.html&quot;&gt;https:&#x2F;&#x2F;lamport.azurewebsites.net&#x2F;video&#x2F;intro.html&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
</content>
	</entry>
	<entry xml:lang="en">
		<title>Solve Wordle Using Information Theory</title>
		<published>2022-02-06T17:05:33.089+00:00</published>
		<updated>2022-02-06T17:05:33.089+00:00</updated>
		<link rel="alternate" href="https://www.thomasantony.com/notes/202201061105-solve-wordle-using-information-theory/" type="text/html"/>
		<id>https://www.thomasantony.com/notes/202201061105-solve-wordle-using-information-theory/</id>
		<content type="html">&lt;p&gt;This is a note about how to use information theory to create an optimal strategy for solving “Wordle”.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;initial-ideas&quot;&gt;Initial ideas&lt;&#x2F;h2&gt;
&lt;p&gt;For any given word, we can find a distribution for probabilities for each pattern of colors that could be revealed. Example, for “WEARY”, there is some number of words that do not contain any of the letters, some that contain W, some with W and E, some with E at the first position, etc.&lt;&#x2F;p&gt;
&lt;p&gt;We can geenrate the list of all possible outcomes for a given word, and check how many words in the list match the patterns to come up with a probability.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Key&lt;&#x2F;strong&gt; A guess is “informative” if it is unlikely, i.e., the more unlikely the  pattern, the fewer possibilities remain to be checked. A highly unlikely guess, if correct, significantly cuts down the solution space.&lt;&#x2F;p&gt;
&lt;p&gt;The expected amount of information from the distribution for a certain word is &lt;&#x2F;p&gt;
&lt;p&gt;$$
E[\text{Information}] = \sum_x {p(x) \cdot \text{Something}}
$$&lt;&#x2F;p&gt;
&lt;p&gt;This “something” should measure how informative that pattern is. We could try to lower the average number of matches. The fewer the number of matches for a pattern, the more informative that pattern is. Instead we can use a concept from information theory.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-is-a-bit&quot;&gt;What is a “bit”?&lt;&#x2F;h2&gt;
&lt;p&gt;An observation that can cut the space of possibilities in half, that is called a bit of information.&lt;&#x2F;p&gt;
&lt;p&gt;Therefore&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#2b303b;color:#c0c5ce;&quot;&gt;&lt;code&gt;&lt;span&gt;p(1 bit) = 1&#x2F;2
&lt;&#x2F;span&gt;&lt;span&gt;p(2 bits) = 1&#x2F;4
&lt;&#x2F;span&gt;&lt;span&gt;p(3 bits) = 1&#x2F;8 etc.
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Or, for information $I$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\left(\frac{1}{2}\right)^I = p \implies 2^I = \frac{1}{p}\
I = \log_2{\left(\frac{1}{p}\right)}
$$&lt;&#x2F;p&gt;
&lt;p&gt;This is a better way of representing probability. You can say that you get 20 bits of information instead of saying that the probability of such a thing occuring is 0.00000095.&lt;&#x2F;p&gt;
&lt;p&gt;Information (or bits) add together the same way probabilities multiply, thanks to the logarithm.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;applying-information-theory-to-the-problem&quot;&gt;Applying information theory to the problem&lt;&#x2F;h2&gt;
&lt;p&gt;$$
E[\text{Information}] = \sum_x {p(x) \cdot \log_2{\left(\frac{1}{p(x)}\right)}
}
$$&lt;&#x2F;p&gt;
&lt;p&gt;This is the average number of bits you could get from a  certain word. The higher the probability of seeing a pattern, the lower the number of bits.&lt;&#x2F;p&gt;
&lt;p&gt;This metric can be used to rank the guess in order of number of expected bits of information.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;**This expected value of information is also called “entropy” ($H$). This was a suggestion by von Neumann to Claude Shannon. While this has some connection to the entropy from thermodynamics, Shannon was dealing purely with probability distribution when he used the term. **&lt;&#x2F;p&gt;
&lt;h2 id=&quot;strategy&quot;&gt;Strategy&lt;&#x2F;h2&gt;
&lt;p&gt;First give a guess based on the word that gives the highest expected number of bits of information. Then for the next guess use the more restricted list of words based on the result of the first guess, and so on.&lt;&#x2F;p&gt;
&lt;p&gt;The solver can be run across every single word in the word-list to work out an average score.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;a-better-strategy&quot;&gt;A better strategy?&lt;&#x2F;h2&gt;
&lt;p&gt;Assign a probability for each word for being in the final list based on frequency of use in the English language instead of a uniform distribution. Grant ranks them by frequency and uses a sigmoid function to assign probabilities. Use this to measure the “remaining uncertainty” of the possible word candidates left. So even if the number of possible words is higher, the uncertainty would be lower thanks to the weighted distribution. &lt;&#x2F;p&gt;
&lt;h2 id=&quot;expected-score&quot;&gt;Expected Score&lt;&#x2F;h2&gt;
&lt;p&gt;TBD.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;p&gt;[1] &lt;a rel=&quot;noopener&quot; target=&quot;_blank&quot; href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=v68zYyaEmEA&quot;&gt;The mathematically optimal Wordle strategy&lt;&#x2F;a&gt;, 3Blue1Brown&lt;&#x2F;p&gt;
</content>
	</entry>
</feed>
